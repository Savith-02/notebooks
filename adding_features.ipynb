{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Savith-02/notebooks/blob/main/adding_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, RepeatVector\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def setup_gpu():\n",
        "    physical_devices = tf.config.list_physical_devices('GPU')\n",
        "    if physical_devices:\n",
        "        print(f\"Found {len(physical_devices)} GPU(s):\")\n",
        "        for i, device in enumerate(physical_devices):\n",
        "            print(f\"  GPU {i}: {device.name}\")\n",
        "\n",
        "        try:\n",
        "            for device in physical_devices:\n",
        "                tf.config.experimental.set_memory_growth(device, True)\n",
        "            print(\"GPU memory growth enabled\")\n",
        "\n",
        "            tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "            print(\"Mixed precision policy set to mixed_float16\")\n",
        "\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error configuring GPU: {str(e)}\")\n",
        "            return False\n",
        "    else:\n",
        "        print(\"No GPU found. Using CPU.\")\n",
        "        return False\n",
        "\n",
        "is_using_gpu = setup_gpu()\n",
        "\n",
        "SEED = 42\n",
        "def set_seeds(seed_value):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    tf.random.set_seed(seed_value)\n",
        "\n",
        "    if tf.config.list_physical_devices('GPU'):\n",
        "        try:\n",
        "            os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "            os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "            tf.config.experimental.enable_op_determinism()\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not enable deterministic operations in TensorFlow: {e}\")\n",
        "\n",
        "set_seeds(SEED)\n",
        "print(f\"Seed set to {SEED}\")\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1DsKaiLMxsT-VHKwqm4vvtANCIjadi1-r'\n",
        "df = pd.read_csv(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Blow_2bfWBBT",
        "outputId": "76162078-47b4-436f-baa8-d080c0e9ca48"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 GPU(s):\n",
            "  GPU 0: /physical_device:GPU:0\n",
            "GPU memory growth enabled\n",
            "Mixed precision policy set to mixed_float16\n",
            "Seed set to 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_kingdom_id(df):\n",
        "    unique_kingdoms = df['kingdom'].unique()\n",
        "    kingdom_to_id = {kingdom: i + 1 for i, kingdom in enumerate(unique_kingdoms)}\n",
        "    df['kingdom_ID'] = df['kingdom'].map(kingdom_to_id)\n",
        "    return df\n",
        "\n",
        "df = add_kingdom_id(df)"
      ],
      "metadata": {
        "id": "na2Oh92LW7QH"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration parameters\n",
        "EPOCHS = 55\n",
        "BATCH_SIZE = 32\n",
        "SEQ_LENGTH = 60\n",
        "PRED_LENGTH = 8\n"
      ],
      "metadata": {
        "id": "1NZ3Uy6DXmgW"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def prepare_data(df):\n",
        "    end_year, end_month, end_day = map(int, TRAINING_DATE_END.split('-'))\n",
        "    df = df.dropna(subset=['kingdom'])\n",
        "\n",
        "    numeric_columns = ['Avg_Temperature','Radiation', 'Rain_Amount',\n",
        "                      'Wind_Speed', 'Wind_Direction',\n",
        "                       'latitude', 'longitude', 'Year', 'Month', 'Day']\n",
        "\n",
        "    for col in numeric_columns:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    for col in numeric_columns:\n",
        "        if col in df.columns and df[col].isna().any():\n",
        "            if col in ['Year', 'Month', 'Day']:\n",
        "                df = df.dropna(subset=[col])\n",
        "            else:\n",
        "                median_val = df[col].median()\n",
        "                df[col] = df[col].fillna(median_val)\n",
        "                print(f\"Filled NaN values in {col} with median: {median_val}\")\n",
        "\n",
        "    date_mask = ((df['Year'] < end_year) |\n",
        "                ((df['Year'] == end_year) & (df['Month'] < end_month)) |\n",
        "                ((df['Year'] == end_year) & (df['Month'] == end_month) & (df['Day'] <= end_day)))\n",
        "\n",
        "    df = df[date_mask]\n",
        "    print(f\"Data filtered until date: Year {end_year}, Month {end_month}, Day {end_day}\")\n",
        "\n",
        "    kingdoms = sorted(df['kingdom'].unique())[:NUM_KINGDOMS]\n",
        "    kingdom_to_id = {kingdom: idx + 1 for idx, kingdom in enumerate(kingdoms)}\n",
        "\n",
        "    df = df[df['kingdom'].isin(kingdoms)]\n",
        "    df['kingdom_ID'] = df['kingdom'].map(kingdom_to_id)\n",
        "\n",
        "    kingdom_info = {}\n",
        "    for kingdom in kingdoms:\n",
        "        k_data = df[df['kingdom'] == kingdom].iloc[0]\n",
        "        kingdom_info[kingdom] = {\n",
        "            'id': int(k_data['kingdom_ID']),\n",
        "            'latitude': float(k_data['latitude']),\n",
        "            'longitude': float(k_data['longitude'])\n",
        "        }\n",
        "\n",
        "    print(f\"Found {len(kingdoms)} kingdoms in the data\")\n",
        "    return df, kingdoms, kingdom_info\n",
        "\n",
        "def scale_features(data, features):\n",
        "    scalers = {}\n",
        "    scaled_data = data.copy()\n",
        "    non_scaled_features = ['Year', 'Month', 'Day', 'kingdom_ID']\n",
        "\n",
        "    for feature in features:\n",
        "        if feature in non_scaled_features:\n",
        "            scalers[feature] = None\n",
        "            continue\n",
        "\n",
        "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "        scaled_data[feature] = scaler.fit_transform(data[feature].values.reshape(-1, 1))\n",
        "        scalers[feature] = scaler\n",
        "\n",
        "    return scaled_data, scalers\n",
        "\n",
        "def create_sequences_optimized(data, input_features, output_features, seq_length=SEQ_LENGTH, pred_length=PRED_LENGTH):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "\n",
        "    for kingdom_id in data['kingdom_ID'].unique():\n",
        "        kingdom_data = data[data['kingdom_ID'] == kingdom_id].copy()\n",
        "        kingdom_data.loc[:, 'date_tuple'] = list(zip(kingdom_data['Year'], kingdom_data['Month'], kingdom_data['Day']))\n",
        "        unique_dates = sorted(kingdom_data['date_tuple'].unique())\n",
        "\n",
        "        if len(unique_dates) < seq_length + pred_length:\n",
        "            print(f\"Not enough data for kingdom ID {kingdom_id}\")\n",
        "            continue\n",
        "\n",
        "        date_to_row = {}\n",
        "        for _, row in kingdom_data.iterrows():\n",
        "            date_tuple = (row['Year'], row['Month'], row['Day'])\n",
        "            date_to_row[date_tuple] = row\n",
        "\n",
        "        for i in range(0, len(unique_dates) - seq_length - pred_length + 1, 8):\n",
        "            seq_dates = unique_dates[i:i+seq_length]\n",
        "            target_dates = unique_dates[i+seq_length:i+seq_length+pred_length]\n",
        "\n",
        "            seq_data = []\n",
        "            target_data = []\n",
        "\n",
        "            all_dates_found = True\n",
        "            for date in seq_dates:\n",
        "                if date in date_to_row:\n",
        "                    seq_data.append(date_to_row[date][input_features].values)\n",
        "                else:\n",
        "                    all_dates_found = False\n",
        "                    break\n",
        "\n",
        "            if not all_dates_found:\n",
        "                continue\n",
        "\n",
        "            all_dates_found = True\n",
        "            for date in target_dates:\n",
        "                if date in date_to_row:\n",
        "                    target_data.append(date_to_row[date][output_features].values)\n",
        "                else:\n",
        "                    all_dates_found = False\n",
        "                    break\n",
        "\n",
        "            if all_dates_found and len(seq_data) == seq_length and len(target_data) == pred_length:\n",
        "                sequences.append(np.array(seq_data))\n",
        "                targets.append(np.array(target_data))\n",
        "\n",
        "    sequences_array = np.array(sequences, dtype=np.float32)\n",
        "    targets_array = np.array(targets, dtype=np.float32)\n",
        "\n",
        "    print(f\"Created {len(sequences)} sequence-target pairs\")\n",
        "    return sequences_array, targets_array\n",
        "\n",
        "\n",
        "def build_model(input_shape, output_timesteps, num_features, lr=0.001):\n",
        "    \"\"\"\n",
        "    Build an improved LSTM model specifically tuned for weather forecasting.\n",
        "    This maintains the same code structure while enhancing performance.\n",
        "    \"\"\"\n",
        "    tf.keras.utils.set_random_seed(SEED)\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=input_shape),\n",
        "\n",
        "        # First LSTM layer with slightly increased capacity\n",
        "        tf.keras.layers.LSTM(\n",
        "            160,  # Slightly increased units (was 128)\n",
        "            return_sequences=False,\n",
        "            kernel_initializer=tf.keras.initializers.GlorotUniform(seed=SEED),\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(1e-6)  # Very light regularization\n",
        "        ),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "        # Repeat vector for output sequence generation\n",
        "        tf.keras.layers.RepeatVector(output_timesteps),\n",
        "\n",
        "        # Second LSTM layer with BatchNorm and Dropout\n",
        "        tf.keras.layers.LSTM(\n",
        "            160,  # Increased units (was 128)\n",
        "            return_sequences=True,\n",
        "            kernel_initializer=tf.keras.initializers.GlorotUniform(seed=SEED),\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(1e-6)  # Very light regularization\n",
        "        ),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "        # Third LSTM layer for further refinement\n",
        "        tf.keras.layers.LSTM(\n",
        "            80,  # Increased units (was 64)\n",
        "            return_sequences=True,\n",
        "            kernel_initializer=tf.keras.initializers.GlorotUniform(seed=SEED),\n",
        "            recurrent_dropout=0.1  # Add recurrent dropout to reduce overfitting\n",
        "        ),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "        # Output layer with activation suitable for weather data (always positive)\n",
        "        tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(num_features))\n",
        "    ])\n",
        "\n",
        "    # Improved learning rate schedule for weather forecasting\n",
        "    # - Starts with higher learning rate for faster initial convergence\n",
        "    # - Gradually decreases to fine-tune predictions\n",
        "    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
        "        initial_learning_rate=lr * 1.5,  # Start with higher learning rate\n",
        "        decay_steps=1500,                # Gradual decay\n",
        "        alpha=0.05                       # Don't let it get too small\n",
        "    )\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(\n",
        "        learning_rate=lr_schedule,\n",
        "        clipnorm=1.0  # Add gradient clipping to improve stability\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='mean_squared_error'\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# def train_global_model(X, y, scalers, patience=8, val_ratio=0.2):\n",
        "#     print(\"\\n===== Training Global Model for All Kingdoms =====\")\n",
        "\n",
        "#     if X.size == 0 or y.size == 0:\n",
        "#         print(\"Error: No training sequences could be created.\")\n",
        "#         return None, None\n",
        "\n",
        "#     # Use sklearn's built-in train_test_split with shuffle=False for time series data\n",
        "#     from sklearn.model_selection import train_test_split\n",
        "#     X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_ratio, shuffle=False)\n",
        "#     print(f\"Training: {X_train.shape}, Validation: {X_val.shape}\")\n",
        "\n",
        "#     # Build model\n",
        "#     input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "#     model = build_model(input_shape, y_train.shape[1], y_train.shape[2])\n",
        "#     model.summary()\n",
        "\n",
        "#     # Create callbacks\n",
        "#     callbacks = [\n",
        "#         # Early stopping\n",
        "#         tf.keras.callbacks.EarlyStopping(\n",
        "#             monitor='val_loss',\n",
        "#             patience=patience,\n",
        "#             restore_best_weights=True,\n",
        "#             verbose=1,\n",
        "#             min_delta=0.0001\n",
        "#         ),\n",
        "#             # Model checkpointing - save best model\n",
        "#             tf.keras.callbacks.ModelCheckpoint(\n",
        "#             filepath='best_model.keras',  # use filepath instead\n",
        "#             monitor='val_loss',\n",
        "#             save_best_only=True,\n",
        "#             verbose=1\n",
        "#         )\n",
        "#     ]\n",
        "\n",
        "#     # Train with validation\n",
        "#     batch_size = BATCH_SIZE * 2 if is_using_gpu else BATCH_SIZE\n",
        "#     print(f\"Using batch size: {batch_size}\")\n",
        "\n",
        "#     history = model.fit(\n",
        "#         X_train, y_train,\n",
        "#         validation_data=(X_val, y_val),\n",
        "#         epochs=EPOCHS,\n",
        "#         batch_size=batch_size,\n",
        "#         callbacks=callbacks,\n",
        "#         verbose=1\n",
        "#     )\n",
        "\n",
        "#     # Evaluate final model\n",
        "#     print(\"\\n===== Final Model Evaluation =====\")\n",
        "#     train_preds = model.predict(X_train, verbose=0)\n",
        "#     val_preds = model.predict(X_val, verbose=0)\n",
        "\n",
        "#     train_mse, train_smape, _, _ = evaluate_predictions(y_train, train_preds, scalers)\n",
        "#     val_mse, val_smape, _, _ = evaluate_predictions(y_val, val_preds, scalers)\n",
        "\n",
        "#     print(f\"Final MSE  - Train: {train_mse:.4f}, Validation: {val_mse:.4f}\")\n",
        "#     print(f\"Final SMAPE - Train: {train_smape:.2f}%, Validation: {val_smape:.2f}%\")\n",
        "\n",
        "#     # Plot metrics\n",
        "#     plt.figure(figsize=(15, 5))\n",
        "\n",
        "#     # Plot loss curves\n",
        "#     plt.subplot(1, 3, 1)\n",
        "#     plt.plot(history.history['loss'], 'b-', label='Train')\n",
        "#     plt.plot(history.history['val_loss'], 'r-', label='Validation')\n",
        "#     plt.title('Model Loss')\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.ylabel('Loss')\n",
        "#     plt.grid(True, alpha=0.3)\n",
        "#     plt.legend()\n",
        "\n",
        "#     # Plot learning rate if available\n",
        "#     if 'lr' in history.history:\n",
        "#         plt.subplot(1, 3, 2)\n",
        "#         plt.plot(history.history['lr'], 'g-', label='Learning Rate')\n",
        "#         plt.title('Learning Rate')\n",
        "#         plt.xlabel('Epoch')\n",
        "#         plt.ylabel('Learning Rate')\n",
        "#         plt.grid(True, alpha=0.3)\n",
        "#         plt.legend()\n",
        "\n",
        "#     # Save the trained model\n",
        "#     model.save('kingdom_weather_model.keras')\n",
        "#     print(\"Model saved to kingdom_weather_model.keras in Keras format\")\n",
        "\n",
        "#     # Visualize SMAPE on feature-by-feature basis for validation set\n",
        "#     features_to_plot = min(5, y_val.shape[2])\n",
        "#     fig, axes = plt.subplots(1, features_to_plot, figsize=(15, 4))\n",
        "\n",
        "#     for i in range(features_to_plot):\n",
        "#         feature_name = OUTPUT_FEATURES[i] if i < len(OUTPUT_FEATURES) else f\"Feature {i}\"\n",
        "\n",
        "#         # Get feature-specific values\n",
        "#         y_true = y_val[:, :, i].flatten()\n",
        "#         y_pred = val_preds[:, :, i].flatten()\n",
        "\n",
        "#         # Inverse transform if scaler exists\n",
        "#         if OUTPUT_FEATURES[i] in scalers and scalers[OUTPUT_FEATURES[i]] is not None:\n",
        "#             scaler = scalers[OUTPUT_FEATURES[i]]\n",
        "#             y_true = scaler.inverse_transform(y_true.reshape(-1, 1)).flatten()\n",
        "#             y_pred = scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
        "\n",
        "#         # Calculate feature-specific SMAPE\n",
        "#         feature_smape = smape(y_true, y_pred)\n",
        "\n",
        "#         # Plot actual vs predicted for this feature (sample points)\n",
        "#         max_points = 100  # Limit points to avoid overcrowding\n",
        "#         indices = np.linspace(0, len(y_true)-1, min(max_points, len(y_true))).astype(int)\n",
        "\n",
        "#         if features_to_plot > 1:\n",
        "#             ax = axes[i]\n",
        "#         else:\n",
        "#             ax = axes\n",
        "\n",
        "#         ax.plot(y_true[indices], label='Actual', marker='o', markersize=4, linestyle='', alpha=0.7)\n",
        "#         ax.plot(y_pred[indices], label='Predicted', marker='x', markersize=4, linestyle='', alpha=0.7)\n",
        "#         ax.set_title(f\"{feature_name}\\nSMAPE: {feature_smape:.2f}%\")\n",
        "#         ax.grid(True, alpha=0.3)\n",
        "\n",
        "#         if i == 0:  # Only add legend to first subplot\n",
        "#             ax.legend()\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.savefig('model_evaluation.png', dpi=300)\n",
        "#     plt.close()\n",
        "\n",
        "#     return model, history\n",
        "def train_global_model(X, y, scalers, patience=7):\n",
        "    print(\"\\n===== Training Global Model for All Kingdoms =====\")\n",
        "\n",
        "    if X.size == 0 or y.size == 0:\n",
        "        print(\"Error: No training sequences could be created.\")\n",
        "        return None, None\n",
        "\n",
        "    # Use all available data for training (no validation split)\n",
        "    print(f\"Using full dataset for training: {X.shape}\")\n",
        "\n",
        "    # Build model\n",
        "    input_shape = (X.shape[1], X.shape[2])\n",
        "    model = build_model(input_shape, y.shape[1], y.shape[2])\n",
        "    model.summary()\n",
        "\n",
        "    # Create callbacks\n",
        "    callbacks = [\n",
        "        # No early stopping based on validation data\n",
        "        # Just use ModelCheckpoint to save the final model\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath='kingdom_weather_model.keras',\n",
        "            save_best_only=False,  # Save the final model\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Train with all data\n",
        "    batch_size = BATCH_SIZE * 2 if is_using_gpu else BATCH_SIZE\n",
        "    print(f\"Using batch size: {batch_size}\")\n",
        "\n",
        "    history = model.fit(\n",
        "        X, y,\n",
        "        epochs=EPOCHS,  # Use a fixed number of epochs\n",
        "        batch_size=batch_size,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate final model\n",
        "    print(\"\\n===== Final Model Evaluation =====\")\n",
        "    train_preds = model.predict(X, verbose=0)\n",
        "    train_mse, train_smape, _, _ = evaluate_predictions(y, train_preds, scalers)\n",
        "\n",
        "    print(f\"Final MSE  - Train: {train_mse:.4f}\")\n",
        "    print(f\"Final SMAPE - Train: {train_smape:.2f}%\")\n",
        "\n",
        "    # Save the trained model\n",
        "    model.save('kingdom_weather_model.keras')\n",
        "    print(\"Model saved to kingdom_weather_model.keras in Keras format\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def evaluate_predictions(y_true, y_pred, scalers):\n",
        "    if y_true.size == 0 or y_pred.size == 0:\n",
        "        return float('nan'), float('nan'), None, None\n",
        "\n",
        "    y_pred_unscaled = np.zeros_like(y_pred)\n",
        "    y_true_unscaled = np.zeros_like(y_true)\n",
        "\n",
        "    for i, feature in enumerate(OUTPUT_FEATURES):\n",
        "        scaler = scalers[feature]\n",
        "        if scaler is not None:\n",
        "            y_pred_unscaled[:, :, i] = scaler.inverse_transform(y_pred[:, :, i])\n",
        "            y_true_unscaled[:, :, i] = scaler.inverse_transform(y_true[:, :, i])\n",
        "        else:\n",
        "            y_pred_unscaled[:, :, i] = y_pred[:, :, i]\n",
        "            y_true_unscaled[:, :, i] = y_true[:, :, i]\n",
        "\n",
        "    mse = mean_squared_error(y_true_unscaled.flatten(), y_pred_unscaled.flatten())\n",
        "    smape_val = smape(y_true_unscaled.flatten(), y_pred_unscaled.flatten())\n",
        "\n",
        "    return mse, smape_val, y_true_unscaled, y_pred_unscaled\n"
      ],
      "metadata": {
        "id": "RDlEuNvIXaVO"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to add the engineered features\n",
        "def add_engineered_features(df):\n",
        "    # Sort by kingdom and date to ensure correct calculation\n",
        "    df = df.sort_values(by=['kingdom_ID', 'Year', 'Month', 'Day']).copy()\n",
        "\n",
        "    # Group by kingdom for feature engineering\n",
        "    grouped = df.groupby('kingdom_ID')\n",
        "\n",
        "    # Features to apply engineering to\n",
        "    base_features = ['Avg_Temperature', 'Radiation', 'Rain_Amount', 'Wind_Speed', 'Wind_Direction']\n",
        "\n",
        "    # Add rolling min/max for different windows\n",
        "    windows = [3, 7, 14]\n",
        "    for window in windows:\n",
        "        for feature in base_features:\n",
        "            min_col = f'{feature}_rollmin_{window}'\n",
        "            max_col = f'{feature}_rollmax_{window}'\n",
        "            df[min_col] = grouped[feature].transform(lambda x: x.rolling(window, min_periods=1).min())\n",
        "            df[max_col] = grouped[feature].transform(lambda x: x.rolling(window, min_periods=1).max())\n",
        "\n",
        "    # Fill any NaN values that might have been created\n",
        "    for col in df.columns:\n",
        "        if df[col].isna().any():\n",
        "            # First try backfill\n",
        "            df[col] = df[col].fillna(method='bfill')\n",
        "            # Then try forward fill\n",
        "            df[col] = df[col].fillna(method='ffill')\n",
        "            # If still NaN, use median\n",
        "            if df[col].isna().any():\n",
        "                df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "    return df\n",
        "\n",
        "# Modified recursive_forecast to handle engineered features for prediction\n",
        "def recursive_forecast_with_features(model, data, kingdom, start_date, num_predictions, kingdom_info, scalers, seq_length=SEQ_LENGTH, pred_length=None):\n",
        "    print(f\"\\n===== Starting Recursive Forecast for {kingdom} =====\")\n",
        "    print(f\"Requested {num_predictions} total days of predictions\")\n",
        "\n",
        "    k_id = kingdom_info[kingdom]['id']\n",
        "    lat = kingdom_info[kingdom]['latitude']\n",
        "    lon = kingdom_info[kingdom]['longitude']\n",
        "\n",
        "    kingdom_data = data[data['kingdom'] == kingdom].sort_values(by=['Year', 'Month', 'Day'])\n",
        "    print(f\"Found {len(kingdom_data)} records for {kingdom}\")\n",
        "\n",
        "    if len(kingdom_data) == 0:\n",
        "        print(f\"Error: No data found for kingdom '{kingdom}' (ID={k_id})\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    min_date = (int(kingdom_data['Year'].min()), int(kingdom_data['Month'].min()), int(kingdom_data['Day'].min()))\n",
        "    max_date = (int(kingdom_data['Year'].max()), int(kingdom_data['Month'].max()), int(kingdom_data['Day'].max()))\n",
        "    print(f\"Date range for {kingdom}: {min_date} to {max_date}\")\n",
        "    print(f\"Attempting to start predictions from: {start_date}\")\n",
        "\n",
        "    start_year, start_month, start_day = start_date\n",
        "\n",
        "    filtered_data = kingdom_data[\n",
        "        (kingdom_data['Year'] == start_year) &\n",
        "        (kingdom_data['Month'] == start_month) &\n",
        "        (kingdom_data['Day'] == start_day)\n",
        "    ]\n",
        "\n",
        "    if len(filtered_data) == 0:\n",
        "        print(f\"Error: Start date {start_date} not found for {kingdom}. Checking nearby dates...\")\n",
        "\n",
        "        dates_in_month = kingdom_data[\n",
        "            (kingdom_data['Year'] == start_year) &\n",
        "            (kingdom_data['Month'] == start_month)\n",
        "        ]\n",
        "\n",
        "        if len(dates_in_month) > 0:\n",
        "            closest_day = dates_in_month['Day'].iloc[0]\n",
        "            for day in dates_in_month['Day']:\n",
        "                if abs(day - start_day) < abs(closest_day - start_day):\n",
        "                    closest_day = day\n",
        "\n",
        "            print(f\"Found closest day: {start_year}-{start_month}-{closest_day}\")\n",
        "            start_day = closest_day\n",
        "            start_date = (start_year, start_month, start_day)\n",
        "\n",
        "            filtered_data = kingdom_data[\n",
        "                (kingdom_data['Year'] == start_year) &\n",
        "                (kingdom_data['Month'] == start_month) &\n",
        "                (kingdom_data['Day'] == start_day)\n",
        "            ]\n",
        "        else:\n",
        "            print(f\"No dates found in month {start_year}-{start_month} for {kingdom}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    if len(filtered_data) == 0:\n",
        "        print(f\"Error: Still could not find suitable start date for {kingdom}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    start_idx = filtered_data.index[0]\n",
        "    all_indices = kingdom_data.index.tolist()\n",
        "\n",
        "    try:\n",
        "        position = all_indices.index(start_idx)\n",
        "    except ValueError:\n",
        "        print(f\"Error: Could not find start index in kingdom data indices\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    if position < seq_length - 1:\n",
        "        print(f\"Warning: Not enough history before {start_date} for {kingdom}. Using available data.\")\n",
        "        initial_data = kingdom_data.iloc[:position+1]\n",
        "    else:\n",
        "        initial_data = kingdom_data.iloc[position-seq_length+1:position+1]\n",
        "\n",
        "    print(f\"Initial sequence has {len(initial_data)} days of data\")\n",
        "\n",
        "    if len(initial_data) < 1:\n",
        "        print(f\"Error: No data available for {kingdom}. Skipping.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    results = []\n",
        "    working_data = initial_data.copy()\n",
        "\n",
        "    round_idx = 0\n",
        "    while len(results) < num_predictions:\n",
        "        round_idx += 1\n",
        "        print(f\"Prediction round {round_idx} for {kingdom}\")\n",
        "\n",
        "        # Ensure working_data has all the engineered features\n",
        "        working_data = add_engineered_features(working_data)\n",
        "\n",
        "        latest_data = working_data.tail(seq_length)\n",
        "\n",
        "        if len(latest_data) < seq_length:\n",
        "            print(f\"Warning: Using {len(latest_data)} days instead of {seq_length} days for sequence.\")\n",
        "\n",
        "        latest_min_date = (\n",
        "            int(latest_data['Year'].min()),\n",
        "            int(latest_data['Month'].min()),\n",
        "            int(latest_data['Day'].min())\n",
        "        )\n",
        "        latest_max_date = (\n",
        "            int(latest_data['Year'].max()),\n",
        "            int(latest_data['Month'].max()),\n",
        "            int(latest_data['Day'].max())\n",
        "        )\n",
        "        print(f\"  Input sequence covers: {latest_min_date} to {latest_max_date}\")\n",
        "\n",
        "        X_seq = []\n",
        "        for _, row in latest_data.iterrows():\n",
        "            scaled_row = row.copy()\n",
        "\n",
        "            for feature in FINAL_INPUT_FEATURES:\n",
        "                if feature not in ['Year', 'Month', 'Day', 'kingdom_ID']:\n",
        "                    if scalers[feature] is not None:\n",
        "                        scaled_row[feature] = scalers[feature].transform([[row[feature]]])[0][0]\n",
        "\n",
        "            X_seq.append(scaled_row[FINAL_INPUT_FEATURES].values)\n",
        "\n",
        "        X_seq = np.array([X_seq], dtype=np.float32)\n",
        "        print(f\"  Input sequence shape: {X_seq.shape}\")\n",
        "\n",
        "        y_pred = model.predict(X_seq, verbose=0)\n",
        "        print(f\"  Prediction output shape: {y_pred.shape}\")\n",
        "\n",
        "        model_pred_length = y_pred.shape[1]\n",
        "        if pred_length is None:\n",
        "            pred_length = model_pred_length\n",
        "            print(f\"  Using model's output length: {pred_length} days\")\n",
        "\n",
        "        y_pred_unscaled = np.zeros_like(y_pred)\n",
        "        for i, feature in enumerate(OUTPUT_FEATURES):\n",
        "            scaler = scalers[feature]\n",
        "            if scaler is not None:\n",
        "                y_pred_unscaled[:, :, i] = scaler.inverse_transform(y_pred[:, :, i])\n",
        "            else:\n",
        "                y_pred_unscaled[:, :, i] = y_pred[:, :, i]\n",
        "\n",
        "        last_date = (\n",
        "            working_data.iloc[-1]['Year'],\n",
        "            working_data.iloc[-1]['Month'],\n",
        "            working_data.iloc[-1]['Day']\n",
        "        )\n",
        "        print(f\"  Last date in working data: {last_date}\")\n",
        "\n",
        "        pred_dates = []\n",
        "        current_year, current_month, current_day = last_date\n",
        "\n",
        "        for day_idx in range(model_pred_length):\n",
        "            current_day += 1\n",
        "\n",
        "            days_in_month = pd.Timestamp(year=int(current_year), month=int(current_month), day=1).days_in_month\n",
        "            if current_day > days_in_month:\n",
        "                current_day = 1\n",
        "                current_month += 1\n",
        "                if current_month > 12:\n",
        "                    current_month = 1\n",
        "                    current_year += 1\n",
        "\n",
        "            pred_dates.append((current_year, current_month, current_day))\n",
        "\n",
        "        print(f\"  Generated {len(pred_dates)} prediction dates from {pred_dates[0]} to {pred_dates[-1]}\")\n",
        "\n",
        "        remaining_preds = num_predictions - len(results)\n",
        "        print(f\"  Remaining predictions needed: {remaining_preds}\")\n",
        "\n",
        "        will_crop = (remaining_preds < model_pred_length)\n",
        "        if will_crop:\n",
        "            print(f\"  Will use only the first {remaining_preds} of {model_pred_length} predictions from this batch\")\n",
        "\n",
        "        use_count = min(model_pred_length, remaining_preds)\n",
        "\n",
        "        for i in range(use_count):\n",
        "            date_tuple = pred_dates[i]\n",
        "            year, month, day = date_tuple\n",
        "\n",
        "            new_row = {\n",
        "                'kingdom': kingdom,\n",
        "                'kingdom_ID': k_id,\n",
        "                'Year': int(year),\n",
        "                'Month': int(month),\n",
        "                'Day': int(day),\n",
        "                'latitude': lat,\n",
        "                'longitude': lon\n",
        "            }\n",
        "\n",
        "            for j, feature in enumerate(OUTPUT_FEATURES):\n",
        "                new_row[feature] = y_pred_unscaled[0, i, j]\n",
        "\n",
        "            results.append(new_row)\n",
        "            working_data = pd.concat([working_data, pd.DataFrame([new_row])], ignore_index=True)\n",
        "\n",
        "        print(f\"  Added {use_count} new predictions\")\n",
        "        print(f\"  Total predictions so far: {len(results)} / {num_predictions}\")\n",
        "\n",
        "        if len(results) >= num_predictions:\n",
        "            print(f\"  Reached target number of predictions ({num_predictions}). Stopping.\")\n",
        "            break\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    print(f\"Generated {len(results_df)} predictions for {kingdom}\")\n",
        "\n",
        "    if len(results_df) > num_predictions:\n",
        "        print(f\"Warning: Generated {len(results_df)} predictions, but only {num_predictions} were requested.\")\n",
        "        print(f\"Trimming to exact requested number.\")\n",
        "        results_df = results_df.head(num_predictions)\n",
        "    elif len(results_df) < num_predictions:\n",
        "        print(f\"Warning: Only generated {len(results_df)} predictions, but {num_predictions} were requested.\")\n",
        "\n",
        "    if len(results_df) > 0:\n",
        "        min_pred_date = (\n",
        "            results_df['Year'].min(),\n",
        "            results_df['Month'].min(),\n",
        "            results_df['Day'].min()\n",
        "        )\n",
        "        max_pred_date = (\n",
        "            results_df['Year'].max(),\n",
        "            results_df['Month'].max(),\n",
        "            results_df['Day'].max()\n",
        "        )\n",
        "        print(f\"Predictions cover {min_pred_date} to {max_pred_date}\")\n",
        "\n",
        "    return results_df\n"
      ],
      "metadata": {
        "id": "kApSpAdYV0rP"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "TRAINING_DATE_END = ('8-12-31')\n",
        "SUBMISSION_DATE_START = ('9-1-1')\n",
        "SUMMISSION_DATE_END = ('9-5-31')\n",
        "\n",
        "NUM_KINGDOMS = 30"
      ],
      "metadata": {
        "id": "uTx6zH2vWw6f"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup GPU if available\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(\"Using GPU\")\n",
        "    for gpu in tf.config.list_physical_devices('GPU'):\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "else:\n",
        "    print(\"Using CPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXBmMmpRYCWv",
        "outputId": "7f2f8721-1a76-4ed5-f099-468cb61a4dd4"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the best features\n",
        "BEST_FEATURES = [\n",
        "    'Avg_Temperature', 'Radiation', 'Rain_Amount', 'Wind_Speed', 'Wind_Direction',\n",
        "    'Avg_Temperature_rollmin_3', 'Avg_Temperature_rollmax_3', 'Radiation_rollmin_3',\n",
        "    'Radiation_rollmax_3', 'Rain_Amount_rollmin_3', 'Rain_Amount_rollmax_3',\n",
        "    'Wind_Speed_rollmin_3', 'Wind_Speed_rollmax_3', 'Wind_Direction_rollmin_3',\n",
        "    'Wind_Direction_rollmax_3', 'Avg_Temperature_rollmin_7', 'Avg_Temperature_rollmax_7',\n",
        "    'Radiation_rollmin_7', 'Radiation_rollmax_7', 'Rain_Amount_rollmin_7',\n",
        "    'Rain_Amount_rollmax_7', 'Wind_Speed_rollmin_7', 'Wind_Speed_rollmax_7',\n",
        "    'Wind_Direction_rollmin_7', 'Wind_Direction_rollmax_7', 'Avg_Temperature_rollmin_14',\n",
        "    'Avg_Temperature_rollmax_14', 'Radiation_rollmin_14', 'Radiation_rollmax_14',\n",
        "    'Rain_Amount_rollmin_14', 'Rain_Amount_rollmax_14', 'Wind_Speed_rollmin_14',\n",
        "    'Wind_Speed_rollmax_14', 'Wind_Direction_rollmin_14', 'Wind_Direction_rollmax_14'\n",
        "]\n",
        "\n",
        "# Define the final input features\n",
        "FINAL_INPUT_FEATURES = ['Year', 'Month', 'Day', 'kingdom_ID', 'latitude', 'longitude'] + BEST_FEATURES\n",
        "\n",
        "OUTPUT_FEATURES = [\n",
        "    'Avg_Temperature', 'Radiation', 'Rain_Amount', 'Wind_Speed', 'Wind_Direction'\n",
        "]"
      ],
      "metadata": {
        "id": "jC-WzIhhnAen"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution code for training the model with best features\n",
        "print(\"Adding engineered features to the dataset...\")\n",
        "engineered_data = add_engineered_features(df)\n",
        "\n",
        "# Prepare data with best features\n",
        "print(\"Preparing data with best features...\")\n",
        "data, kingdoms, kingdom_info = prepare_data(engineered_data)\n",
        "scaled_data, scalers = scale_features(data, FINAL_INPUT_FEATURES)\n",
        "X_train, y_train = create_sequences_optimized(scaled_data, FINAL_INPUT_FEATURES, OUTPUT_FEATURES)"
      ],
      "metadata": {
        "id": "C915i7FfoJaP",
        "outputId": "43cbce94-8361-4890-aaff-c1af401a88f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding engineered features to the dataset...\n",
            "Preparing data with best features...\n",
            "Data filtered until date: Year 8, Month 12, Day 31\n",
            "Found 30 kingdoms in the data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the final model with full dataset and original parameters\n",
        "print(\"Training model with best features...\")\n",
        "model, history = train_global_model(X_train, y_train, scalers)"
      ],
      "metadata": {
        "id": "-Q_YQ3SRoX7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_mode = True"
      ],
      "metadata": {
        "id": "y0XsE9luoiE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SUBMISSION_DATE_START = ('9-1-1')\n",
        "SUMMISSION_DATE_END = ('9-1-31')"
      ],
      "metadata": {
        "id": "9ZWzsIib_KCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now generate predictions using the model with best features\n",
        "if model is None:\n",
        "    print(\"Error: Model training failed. Exiting.\")\n",
        "else:\n",
        "    submission_start = tuple(map(int, str(SUBMISSION_DATE_START).split('-')))\n",
        "    submission_end = tuple(map(int, str(SUMMISSION_DATE_END).split('-')))\n",
        "\n",
        "    if isinstance(TRAINING_DATE_END, str):\n",
        "        training_end = tuple(map(int, TRAINING_DATE_END.split('-')))\n",
        "    else:\n",
        "        training_end = TRAINING_DATE_END\n",
        "\n",
        "    if submission_mode:\n",
        "        start_date = submission_start\n",
        "        end_date = submission_end\n",
        "        print(f\"Submission mode enabled: Predicting from {start_date} to {end_date}\")\n",
        "    else:\n",
        "        year, month, day = training_end\n",
        "        days_in_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
        "\n",
        "        day += 1\n",
        "        if day > days_in_month[month]:\n",
        "            day = 1\n",
        "            month += 1\n",
        "            if month > 12:\n",
        "                month = 1\n",
        "                year += 1\n",
        "\n",
        "        start_date = (year, month, day)\n",
        "\n",
        "        year, month, day = submission_start\n",
        "        day -= 1\n",
        "        if day == 0:\n",
        "            month -= 1\n",
        "            if month == 0:\n",
        "                month = 12\n",
        "                year -= 1\n",
        "            day = days_in_month[month]\n",
        "\n",
        "        end_date = (year, month, day)\n",
        "        print(f\"Testing mode: Predicting from {start_date} to {end_date}\")\n",
        "\n",
        "    def date_to_days(date_tuple):\n",
        "        year, month, day = date_tuple\n",
        "        days_in_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
        "\n",
        "        days = day\n",
        "        for m in range(1, month):\n",
        "            days += days_in_month[m]\n",
        "        days += 365 * year\n",
        "\n",
        "        return days\n",
        "\n",
        "    start_days = date_to_days(start_date)\n",
        "    end_days = date_to_days(end_date)\n",
        "    num_days = end_days - start_days + 1\n",
        "\n",
        "    # Convert start_date and end_date to numeric format for easier filtering\n",
        "    start_date_num = start_date[0] * 10000 + start_date[1] * 100 + start_date[2]\n",
        "    end_date_num = end_date[0] * 10000 + end_date[1] * 100 + end_date[2]\n",
        "\n",
        "    print(f\"Predicting {num_days} days from {start_date} to {end_date}\")\n",
        "    print(f\"Date range in numeric format: {start_date_num} to {end_date_num}\")\n",
        "\n",
        "    all_predictions = []\n",
        "\n",
        "    for kingdom in kingdoms:\n",
        "        if submission_mode:\n",
        "            last_date = training_end\n",
        "        else:\n",
        "            year, month, day = start_date\n",
        "            day -= 1\n",
        "            if day == 0:\n",
        "                month -= 1\n",
        "                if month == 0:\n",
        "                    month = 12\n",
        "                    year -= 1\n",
        "                days_in_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
        "                day = days_in_month[month]\n",
        "\n",
        "            last_date = (year, month, day)\n",
        "\n",
        "        print(f\"Using latest data point for {kingdom}: Year {last_date[0]}, Month {last_date[1]}, Day {last_date[2]}\")\n",
        "\n",
        "        # Use the new recursive forecast function with features\n",
        "        kingdom_predictions = recursive_forecast_with_features(\n",
        "            model, data, kingdom, last_date, num_days, kingdom_info, scalers)\n",
        "\n",
        "        if not kingdom_predictions.empty:\n",
        "            # Create a numeric date column for easier filtering\n",
        "            kingdom_predictions['date_num'] = kingdom_predictions['Year'] * 10000 + kingdom_predictions['Month'] * 100 + kingdom_predictions['Day']\n",
        "\n",
        "            # Filter predictions to match the target date range\n",
        "            filtered_predictions = kingdom_predictions[\n",
        "                (kingdom_predictions['date_num'] >= start_date_num) &\n",
        "                (kingdom_predictions['date_num'] <= end_date_num)\n",
        "            ]\n",
        "\n",
        "            print(f\"Generated {len(kingdom_predictions)} predictions for {kingdom}\")\n",
        "            print(f\"After filtering for target date range: {len(filtered_predictions)} predictions\")\n",
        "\n",
        "            # Drop the temporary date_num column before saving\n",
        "            filtered_predictions = filtered_predictions.drop(columns=['date_num'])\n",
        "\n",
        "            all_predictions.append(filtered_predictions)\n",
        "        else:\n",
        "            print(f\"No predictions generated for {kingdom}\")\n",
        "\n",
        "    if all_predictions:\n",
        "        final_predictions = pd.concat(all_predictions, ignore_index=True)\n",
        "\n",
        "        # Verify we have predictions\n",
        "        print(f\"Total predictions: {len(final_predictions)}\")\n",
        "        if len(final_predictions) == 0:\n",
        "            print(\"WARNING: No predictions survived filtering! Check your date ranges and filtering logic.\")\n",
        "        else:\n",
        "            # Print date range of predictions\n",
        "            min_date = (\n",
        "                int(final_predictions['Year'].min()),\n",
        "                int(final_predictions['Month'].min()),\n",
        "                int(final_predictions['Day'].min())\n",
        "            )\n",
        "            max_date = (\n",
        "                int(final_predictions['Year'].max()),\n",
        "                int(final_predictions['Month'].max()),\n",
        "                int(final_predictions['Day'].max())\n",
        "            )\n",
        "            print(f\"Final predictions cover date range: {min_date} to {max_date}\")\n",
        "\n",
        "            # Save the predictions\n",
        "            output_predictions_path = \"kingdom_weather_predictions_submission.csv\" if submission_mode else \"kingdom_weather_predictions_testing.csv\"\n",
        "            print(f\"Saving predictions to {output_predictions_path}\")\n",
        "            final_predictions.to_csv(output_predictions_path, index=False)\n",
        "            print(\"Prediction process completed!\")\n",
        "    else:\n",
        "        print(\"No predictions were generated for any kingdom.\")"
      ],
      "metadata": {
        "id": "EIXhexIfoGej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DL147pfwJw_O",
        "outputId": "e2a3a80f-8983-4d14-c16c-ad0a635d1379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 0 prediction records\n",
            "Analyzing prediction period: 9-1-1 to 9-5-31\n",
            "Warning: No actual data available for the prediction period.\n",
            "Cannot calculate performance metrics without actual data.\n",
            "Performance evaluation complete!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Evaluation phase\n",
        "if submission_mode:\n",
        "    predictions_file = 'kingdom_weather_predictions_submission.csv'\n",
        "    mode_title = \"Submission\"\n",
        "else:\n",
        "    predictions_file = 'kingdom_weather_predictions_testing.csv'\n",
        "    mode_title = \"Testing\"\n",
        "\n",
        "try:\n",
        "    predictions = pd.read_csv(predictions_file)\n",
        "    print(f\"Loaded {len(predictions)} prediction records\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: {predictions_file} not found.\")\n",
        "    exit(1)\n",
        "\n",
        "historical_data = df\n",
        "\n",
        "if submission_mode:\n",
        "    start_year, start_month, start_day = map(int, SUBMISSION_DATE_START.split('-'))\n",
        "    end_year, end_month, end_day = map(int, SUMMISSION_DATE_END.split('-'))\n",
        "else:\n",
        "    training_end = tuple(map(int, TRAINING_DATE_END.split('-')))\n",
        "    submission_start = tuple(map(int, SUBMISSION_DATE_START.split('-')))\n",
        "\n",
        "    year, month, day = training_end\n",
        "    days_in_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
        "    day += 1\n",
        "    if day > days_in_month[month]:\n",
        "        day = 1\n",
        "        month += 1\n",
        "        if month > 12:\n",
        "            month = 1\n",
        "            year += 1\n",
        "    start_year, start_month, start_day = year, month, day\n",
        "\n",
        "    year, month, day = submission_start\n",
        "    day -= 1\n",
        "    if day == 0:\n",
        "        month -= 1\n",
        "        if month == 0:\n",
        "            month = 12\n",
        "            year -= 1\n",
        "        day = days_in_month[month]\n",
        "    end_year, end_month, end_day = year, month, day\n",
        "\n",
        "print(f\"Analyzing prediction period: {start_year}-{start_month}-{start_day} to {end_year}-{end_month}-{end_day}\")\n",
        "\n",
        "historical_data['date_num'] = historical_data['Year'] * 10000 + historical_data['Month'] * 100 + historical_data['Day']\n",
        "predictions['date_num'] = predictions['Year'] * 10000 + predictions['Month'] * 100 + predictions['Day']\n",
        "\n",
        "start_date_num = start_year * 10000 + start_month * 100 + start_day\n",
        "end_date_num = end_year * 10000 + end_month * 100 + end_day\n",
        "\n",
        "actual_in_pred_period = historical_data[\n",
        "    (historical_data['date_num'] >= start_date_num) &\n",
        "    (historical_data['date_num'] <= end_date_num)\n",
        "]\n",
        "\n",
        "if len(actual_in_pred_period) == 0:\n",
        "    print(\"Warning: No actual data available for the prediction period.\")\n",
        "    print(\"Cannot calculate performance metrics without actual data.\")\n",
        "else:\n",
        "    print(f\"Found {len(actual_in_pred_period)} actual data points in the prediction period.\")\n",
        "    print(\"\\nCalculating overall metrics for all parameters and ALL kingdoms...\")\n",
        "    metrics_data = []\n",
        "\n",
        "    all_kingdoms = sorted(predictions['kingdom'].unique())\n",
        "    print(f\"Calculating metrics for all {len(all_kingdoms)} kingdoms\")\n",
        "\n",
        "    for kingdom in all_kingdoms:\n",
        "        kingdom_pred = predictions[predictions['kingdom'] == kingdom].sort_values('date_num')\n",
        "        kingdom_actual = actual_in_pred_period[actual_in_pred_period['kingdom'] == kingdom].sort_values('date_num')\n",
        "\n",
        "        if len(kingdom_actual) > 0:\n",
        "            for param in OUTPUT_FEATURES:\n",
        "                merged_data = pd.merge(\n",
        "                    kingdom_actual[['date_num', param]],\n",
        "                    kingdom_pred[['date_num', param]],\n",
        "                    on='date_num',\n",
        "                    suffixes=('_actual', '_pred')\n",
        "                )\n",
        "\n",
        "                if len(merged_data) > 0:\n",
        "                    rmse = np.sqrt(mean_squared_error(\n",
        "                        merged_data[f'{param}_actual'],\n",
        "                        merged_data[f'{param}_pred']\n",
        "                    ))\n",
        "\n",
        "                    y_true = merged_data[f'{param}_actual'].values\n",
        "                    y_pred = merged_data[f'{param}_pred'].values\n",
        "                    smape_value = smape(y_true, y_pred)\n",
        "\n",
        "                    metrics_data.append({\n",
        "                        'Kingdom': kingdom,\n",
        "                        'Parameter': param,\n",
        "                        'RMSE': rmse,\n",
        "                        'SMAPE': smape_value,\n",
        "                        'Points': len(merged_data)\n",
        "                    })\n",
        "\n",
        "    if metrics_data:\n",
        "        metrics_df = pd.DataFrame(metrics_data)\n",
        "        metrics_file = f'prediction_metrics_all_{mode_title.lower()}.csv'\n",
        "        metrics_df.to_csv(metrics_file, index=False)\n",
        "        print(f\"Detailed metrics saved to {metrics_file}\")\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.axis('off')\n",
        "\n",
        "        summary = metrics_df.groupby('Parameter').agg({\n",
        "            'RMSE': ['mean', 'min', 'max', 'std'],\n",
        "            'SMAPE': ['mean', 'min', 'max', 'std'],\n",
        "            'Points': 'sum'\n",
        "        })\n",
        "\n",
        "        summary.columns = [f'{col[0]}_{col[1]}' for col in summary.columns]\n",
        "        summary = summary.reset_index()\n",
        "\n",
        "        table_data = []\n",
        "        table_data.append(['Parameter', 'Avg RMSE', 'Min RMSE', 'Max RMSE', 'Std RMSE',\n",
        "                          'Avg SMAPE(%)', 'Min SMAPE(%)', 'Max SMAPE(%)', 'Std SMAPE(%)', 'Data Points'])\n",
        "\n",
        "        for _, row in summary.iterrows():\n",
        "            table_data.append([\n",
        "                row['Parameter'],\n",
        "                f\"{row['RMSE_mean']:.2f}\",\n",
        "                f\"{row['RMSE_min']:.2f}\",\n",
        "                f\"{row['RMSE_max']:.2f}\",\n",
        "                f\"{row['RMSE_std']:.2f}\",\n",
        "                f\"{row['SMAPE_mean']:.2f}%\",\n",
        "                f\"{row['SMAPE_min']:.2f}%\",\n",
        "                f\"{row['SMAPE_max']:.2f}%\",\n",
        "                f\"{row['SMAPE_std']:.2f}%\",\n",
        "                f\"{int(row['Points_sum'])}\"\n",
        "            ])\n",
        "\n",
        "        table = plt.table(\n",
        "            cellText=table_data,\n",
        "            colWidths=[0.12, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09],\n",
        "            loc='center',\n",
        "            cellLoc='center'\n",
        "        )\n",
        "\n",
        "        table.auto_set_font_size(False)\n",
        "        table.set_fontsize(9)\n",
        "        table.scale(1, 1.5)\n",
        "\n",
        "        plt.title(f\"Prediction Performance Metrics - ALL Kingdoms - {mode_title} Period\", fontsize=16)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        summary_file = f'prediction_metrics_summary_{mode_title.lower()}.png'\n",
        "        plt.savefig(summary_file, bbox_inches='tight', dpi=300)\n",
        "        plt.close()\n",
        "        print(f\"Summary metrics visualization saved to {summary_file}\")\n",
        "\n",
        "        overall_rmse = metrics_df['RMSE'].mean()\n",
        "        overall_smape = metrics_df['SMAPE'].mean()\n",
        "\n",
        "        with open(f'overall_metrics_{mode_title.lower()}.txt', 'w') as f:\n",
        "            f.write(f\"Overall Metrics for {mode_title} Period\\n\")\n",
        "            f.write(f\"Total Kingdoms: {len(all_kingdoms)}\\n\")\n",
        "            f.write(f\"Total Weather Parameters: {len(metrics_df['Parameter'].unique())}\\n\")\n",
        "            f.write(f\"Total Data Points: {metrics_df['Points'].sum()}\\n\")\n",
        "            f.write(f\"Overall Average RMSE: {overall_rmse:.4f}\\n\")\n",
        "            f.write(f\"Overall Average SMAPE: {overall_smape:.4f}%\\n\")\n",
        "\n",
        "        print(f\"Overall RMSE across all parameters: {overall_rmse:.4f}\")\n",
        "        print(f\"Overall SMAPE across all parameters: {overall_smape:.4f}%\")\n",
        "    else:\n",
        "        print(\"No metrics data available to create summary table.\")\n",
        "\n",
        "print(\"Performance evaluation complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Post-processing functions\n",
        "try:\n",
        "    predictions = pd.read_csv('kingdom_weather_predictions_testing.csv')\n",
        "    results = predictions.sort_values(by=['Month', 'Day', 'kingdom_ID'])\n",
        "\n",
        "    # Check negative values\n",
        "    for col in results.columns:\n",
        "        if pd.api.types.is_numeric_dtype(results[col]):\n",
        "            negative_count = (results[col] < 0).sum()\n",
        "            print(f\"Column '{col}': {negative_count} negative values\")\n",
        "\n",
        "    # Make negative Rain_Amount values equal to 0\n",
        "    results['Rain_Amount'] = results['Rain_Amount'].apply(lambda x: 0 if x < 0 else x)\n",
        "\n",
        "    # Count zero values\n",
        "    for col in results.columns:\n",
        "        if pd.api.types.is_numeric_dtype(results[col]):\n",
        "            zero_count = (results[col] == 0).sum()\n",
        "            print(f\"Column '{col}': {zero_count} zero values\")\n",
        "\n",
        "    results.to_csv('results_v2_updated.csv', index=False)\n",
        "except FileNotFoundError:\n",
        "    print(\"Warning: 'kingdom_weather_predictions_year.csv' not found. Skipping post-processing.\")"
      ],
      "metadata": {
        "id": "91Yu_QDIKQg6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9a95fc4-a255-4eb7-e263-e988f3ec5f2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column 'kingdom_ID': 0 negative values\n",
            "Column 'Year': 0 negative values\n",
            "Column 'Month': 0 negative values\n",
            "Column 'Day': 0 negative values\n",
            "Column 'latitude': 0 negative values\n",
            "Column 'longitude': 4590 negative values\n",
            "Column 'Avg_Temperature': 0 negative values\n",
            "Column 'Radiation': 350 negative values\n",
            "Column 'Rain_Amount': 1402 negative values\n",
            "Column 'Wind_Speed': 2 negative values\n",
            "Column 'Wind_Direction': 7 negative values\n",
            "Column 'kingdom_ID': 0 zero values\n",
            "Column 'Year': 0 zero values\n",
            "Column 'Month': 0 zero values\n",
            "Column 'Day': 0 zero values\n",
            "Column 'latitude': 0 zero values\n",
            "Column 'longitude': 0 zero values\n",
            "Column 'Avg_Temperature': 0 zero values\n",
            "Column 'Radiation': 0 zero values\n",
            "Column 'Rain_Amount': 1402 zero values\n",
            "Column 'Wind_Speed': 0 zero values\n",
            "Column 'Wind_Direction': 0 zero values\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IxLnN3tpiAJ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}